## Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks

> 2015

### 摘要：

​	目前最好的目标检测网络依赖于区域提案算法来假定目标的位置。如SPPnet和Fast R-CNN等进展减少了这些检测网络的运行时间，使得区域提案计算成为瓶颈。本文中，我们提出一种区域提案网络（$Region\ Proposal\ Network$,RPN），该网络与检测网络共享整幅图像的卷积特征，因次使得区域提案几乎不花费代价。RPN是一个全卷积网络，可以同时的预测目标边界和每个位置的目标分数。RPN通过端到端训练来生成高质量区域提案，可用作Fast R-CNN检测。通过简单的交替优化，RPN和Fast R-CNN可以被训练而共享卷积特征。我们接着通过共享卷积特征将RPN和Fast R-CNN合并到一个单一网络——使用最近流行的具有“attention”机制的神经网络术语，RPN组件会告诉统一的网络去哪里寻找。对于深度VGG-16模型，我们的检测系统在GPU上有5fps的帧率(包括所有步骤)，并在PASCAL VOC 2007 和2012以及MS COCO数据集上达到了目前最好的目标检测准确度（每张图使用300个提案）。在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是多项冠军的基础。代码已经公开可用。

### 1  简介

​	区域提案方法和基于区域的卷积神经网络（R-CNN）的成功驱动了最近目标检测的发展。尽管基于区域的卷积神经网络如最初[5]中构建的在计算上花费很大，它们的代价已经通过在提案间共享卷积[1],[2]而大幅降低。最新的典型代表，Fast R-CNN，使用深度网络达到了几乎实时的比率（当忽略区域提案所花费的时间时）。提案是目前最好的检测系统在测试时的计算瓶颈。

​	区域提案方法一般采用廉价的特征和经济的推理方案。Selective Search[4]，最流行的方法之一，贪婪的合并基于工程低级特征的超像素。然而和高效的检测网络相比[2]，Selective Search要慢一个数量级，在一个CPU实现中速度为每幅图两秒。EdgeBoxes[6]在提案质量和速度上给出了当前最好的权衡，速度为每幅图0.2秒。尽管如此，区域提案这一步骤仍然花费了与检测网络同样多的运行时间。

​	值得注意的一点是快速的基于区域的CNNs受益于GPU，而研究中所用到的区域提案方法都是在CPU上实现的，这使得两者在运行时间的比较是不公平的。一种显而易见的方法来加速提案计算就是将其在GPU上重新实现。这也许是一个工程上高效的解决方法，但是重现实现忽略了下游检测网络因此失去了共享计算的机会。	

​	本文中，我们展示了一个算法上的改变——使用深度卷积神经网络来计算提案——促成了一种高雅而有效的解决方法，在该方法中，提案的计算相比于检测网络的计算几乎是零代价的。为此，我们提出新的，与state-of-the-art目标检测网络共享巻积层的$Region\ Proposal\ Networks$ (RPNS)。通过在测试时共享卷积，计算提案的边缘成本十分小（10ms每幅图）。

​	我们的观察结果表明基于区域的检测器，如Fast R-CNN，所使用的卷积特征图也可以用来生成区域提案。在这些卷积特征之上，我们通过增加一些可以同时回归区域边界和在规则网络中每个位置上的目标分数的巻积层来构建RPN。因此RPN是全卷积网络（FCN）[7]的一种并可以为生成检测提案的任务而专门的进行端到端训练。

![f1](images\f1.png)

​	RPNs被设计用来高效的预测许多不同尺度和宽高比的区域提案。与一般使用图像金字塔（图1，a）或过滤器金字塔（图1，b）的方法[8],[9],[1],[2]相比，我们提出新的“anchor”框作为多尺度和宽高比下的参考。我们的方案可以看作是回归参考金字塔（图1，c），这就避免了对多尺度和宽高比的图像或过滤器的枚举。该模型在使用单一尺度图像时训练和测试都执行的很好，因此对运行速度有利。

​	为了将RPN和Fast R-CNN目标检测网络统一，我们提出了一种在保持提案数固定的条件下，交替对区域提案任务调参和对目标检测任务调参的训练方案。这种方案收敛很快并且会产生一个在两种任务之间共享卷积特征的统一网络。

​	我们在PASCAL VOC检测基准上[11]全面评估了我们的方法，在该基准上使用RPNs的Fast R-CNNs产生了比强基线方法——使用Selective Search的Fast R-CNNs更好的检测准确度。同时，我们的方法在测试时几乎免除了所有Selective Search的计算负担——提案有效的运行时间只有10ms。使用[3]中开销大的深度模型，我们的检测方法在GPU上依然有5fps的帧率（包括所有步骤），因此该方法在速度和准确度上是一个使用的目标检测系统。我们也报告了MS COCO 数据集上的结果并使用COCO 数据探究了在PASCAL VOC 上的提升。代码已经开源：http://github.com/shaoqingren/faster_rcnn(MATLAB)，http://github.com/rbgirshick/py-faster-rcnn (Python)。

​	这篇手稿的初级版本之前在[10]公布。此后，RPN和Faster R-CNN的框架被采用及泛化成了其他方法，如3D目标检测[13]，基于部件的检测[14]，实例分割[15]以及图像标注[16]。随着用户参与度的提高，我们快速有效的目标检测系统也被嵌入如Pinterests[17]的商业系统。

​	在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是在ImageNet检测，ImageNet定位，COCO检测，COCO分割等多个条目中头等奖的基础。RPNs完全是从数据中学习提案区域也因此很容易受益于更深度更具表现力的特征（如[18]中所采用的101层残差网络）。Faster R-CNN和RPN也在这些比赛的其他主要条目上被使用。这些结果表明我们的方法不仅是实际使用中合算的解决方法，也是一种提高目标检测准确度的有效方法。

### 2  相关工作

**目标提案。**关于目标提案方法有大量文献。关于目标提案方法全面的调查和比较可以在[19],[20],[21]中找到。广泛使用的目标提案方法包括基于分组超像素的方法（如Selective Search[4]，CPMC[22]，MCG[23]）以及基于滑窗的方法（如objectness in windows[24]，EdgeBoxes[6]）。目标提案方法作为独立于检测器的外部模块而使用（Selective Search[4]目标检测器，R-CNN[5]以及Fast R-CNN[2]）。

**用于目标检测的深度网络。**R-CNN方法端到端的训练CNNs来将提案区域分类为目标类别或背景。R-CNN主要是作为一个分类器，它并不预测目标边界（除了通过边界框回归来精炼）。它的准确度取决于区域提案模块的性能（见[20]中的比较）。一些论文提出了使用深度网络来预测目标边界框的方法[25],[9],[26],[27]。在OverFeat方法中[9]，一个全连接层被训练用来预测边界框坐标以用作单个物体的定位任务。全连接层接着被转换为巻积层用来检测多个特定类别目标。MultiBox方法[26],[27]从一个网络中生成区域提案，该网络最后的全连接层同时预测多个类别无关的边界框，泛化了OverFeat的“单一框”方式。这些类别无关的边界框被R-CNN用作提案。MultiBox提案网络应用于一个单一图像剪裁或多个大图像剪裁（如224x224），相比于我们全卷积方案。MultiBox在提案和监测网络之间并没有共享特征。在之后介绍我们的方法时会讨论更多关于OverFeat和MultiBox的内容。与我们的工作一起的，DeepMask方法[28]是用来学习分割提案的。

​	卷积的共享计算越来越受到关注，因为它可以高效准确的进行视觉识别。OverFeat从一个用于分类、定位以及检测的图像金字塔中计算卷积特征。共享卷积特征图上的适应性尺寸池化是用来进行高效的基于区域的目标检测和语义分割。Fast R-CNN使得端到端检测器可以在共享卷积特征上训练并展示了令人瞩目的准确度和速度。

### 3  FASTER R-CNN

​	![f2](images\f2.png)

我们的目标检测系统，称为Faster R-CNN，是有两个模块组成的。第一个模块是用来提案区域的全卷积网络，第二个模块是使用提案区域的Fast R-CNN检测器。整个系统是一个用于目标检测的单一，统一的网络（图2）。使用最近流行的具有“attention”机制的网络术语，RPN模块告诉Fast R-CNN模块去哪里寻找。在3.1节中我们将会介绍区域提案网络的设计和属性。在3.2节中我们提出使用共享特征训练两个模块的算法。

#### 3.1  Region Proposal Networks

​	区域提案网络（RPN）使用图像（任何尺寸）作为输入并输出一系列矩形目标提案，每个提案具有一个目标分数。我们使用一个全卷积网络来建模该过程，在本节中我们将会介绍。因为我们最终的目标是与Fast R-CNN目标检测网络共享计算，我们假定两个网络共享一些共同的巻积层。在我们的实验中，我们研究了具有5个可共享巻积层的Zeiler and Fergus模型[32]（ZF）以及具有13个可共享巻积层的Simonyan and Zisserman模型[3]（VGG-16）。

![f3](images\f3.png)

​	为了生成区域提案，我们在最后一个共享巻积层的输出卷积特征图上滑动一个小网络。该网络使用输入卷积特征图上$n\ *\ n$的空间窗口作为输入。每个滑动窗口被映射为一个低维特征（ZF的256-d，VGG的512-d，紧跟着ReLU[33]）。该特征被输入到两个兄弟全连接层——一个边界框回归层（reg）以及一个边界框分类层（cls）。本文中我们使用n=3，注意到输入图像的有效感知野很大（171像素，ZF和228像素，VGG）。该左网络图示如图3（左）。注意到因为小型网络以滑窗的方式运作，全连接层会在所有空间位置上共享。该结构自然的使用一个nxn巻积层紧接着两个兄弟巻积层来实现。

#### 3.1.1 Anchors

​	在每个滑窗的位置，我们同时预测多个区域提案，每个位置最大可能的提案数标记为$k$。所以$\ reg\ $层有$\ 4k\ $个输出来对$\ k\ $个边界框坐标进行编码，而$\ cls\ $层输出$\ 2k\ $个分数以预测每个提案是目标或不是目标的概率。$\ k\ $个提案框相对于$\ k\ $个参考边界框而被参数化，我们称该参考框为$\ anchors\ $。anchor位于所讨论的滑窗中心，并于一个尺度和宽高比相关联（图3左）。默认我们使用3个尺度和3个宽高比，在每个滑动位置上产生$\ k=9\ $ 个anchors。对于$\ W*H\ $的卷积特征图来说，总共会有$\ WHk\ $个anchors。

##### 平移不变的 Anchors

​	我们方法的一个重要属性就是平移不变形（$\ translation\ invariant\ $），无论是在anchors方面还是在计算相对于anchors的提案的函数方面。如果图像中某目标移位，提案也应该以为并且相同的函数应该可以预测任一位置的提案。这种平移不变性受到我们方法的保障。作为对照，MultiBox使用k-means来生成800个anchors，而并不具备平移不变性。所以MultiBox并不能保证某个目标移动的同时产生相同的提案。

​	平移不变性的属性也减少了模型的大小。MultiBox有一个（4+1）\*800-维的全连接层，然而我们的方法只有一个（4+2）\*9-维的卷积输出层（在$\ k=9\ $anchors的情况下）。因此，我们的输出层有2.8\*$10^4$个参数（VGG-16有512\*（4+2）\*9个）比MultiBox的6.1\*$10^6$个参数（GoogleNet有1536\*(4+1)\*800个）少两个数量级。如果考虑特征影射层，我们的提案层依然比MultiBox少一个数量级的参数。我们认为我们的方法在小数据集上（如PASCAL VOC）更不容易过拟合。

##### 多尺度Anchors作为回归参考

​	我们anchors的设计提出了一种处理多尺度（和宽高比）问题的新方案。如图1所示，多尺度预测有两种常用的方法。第一种方法是基于图像/特征金字塔，如DPM和基于CNN的方法。图像被resize到多个尺度，然后对每个尺度计算特征图（HOG，或深度卷积特征）。第二种是很耗时，在特征图上使用多个尺度对滑窗。

​	我们对anchor方法是基于anchor金字塔，比较高效。因为这种多尺度设计，我们可以直接使用单尺度图像单卷积特征。

#### 3.1.2 Loss Function

​	为了训练RPN，我们对每个anchor赋予一个二分类标（是物体 or not）。两类anchor标为positive：1）与某个gt IoU最高的anchor。2）与任何gt IoU大于0.7的anchor。一个gt 可以赋予多个anchor pos类标。与所有gt IoU小于0.3的anchor赋予neg类标。其他anchor对训练没有影响。

​	一副图像的损失函数定义为：
$$
\begin{align*}
L(\{p_i\},\{t_i\}) &= \frac{1}{N_{cls}}\sum_iL_{cls}(p_i,p_i^*)\\
&+\lambda\frac{1}{N_{reg}}\sum_ip^*_iL_{reg}(t_i,t^*_i)
\end{align*}
$$
其中，i表示mini-batch中 anchor 的index。$p_i$为anchor i为目标的概率。gt$\ p^*_i\ $为1或0。$\ t_i\ $是预测的4个坐标的向量，$\ t^*_i\ $与pos anchor相关联的gt的坐标。分类损失$\ L_{cls}\ $是在两个类上的log损失。对于回归损失，我们使用$\ L_{reg}(t_i,t^*_i)=R(t_i-t^*_i)\ $，其中，R为鲁棒损失函数（smooth L1）。$\ p^*_iL_{reg}\ $表示回归损失只对pos anchor有作用。cls和reg层的输出分别包含$\ \{p_i\}\ $和$\ \{t_i\}\ $。

​	这两项由$\ N_{cls}\ $和$\ N_{reg}\ $正则化，由平衡参数$\ \lambda\ $加权。在最新的实现中，$\ cls\ $使用mini-batch-size归一化，$reg$使用anchor位置数量归一化。默认$\ \lambda=10\ $，因此两项损失大致同等重要。

​	对于bb 回归：
$$
\begin{align*}
&t_x = (x-x_a)/w_a,\ \ \ \ \ t_y=(y-y_a)/h_a,\\
&t_w = log(w/w_a),\ \ \ \ \ t_h=log(h/h_a),\\
&t^*_x = (x^*-x_a)/w_a,\ \ \ \ \ t^*_y=(y^*-y_a)/h_a,\\
&t^*_w = log(w^*/w_a),\ \ \ \ \ t^*_h = log(h^*/h_a)
\end{align*}
$$
其中$\ x,y,w,h\ $表示box的中心坐标以及宽高。$\ x,x_a,x^*\ $表示预测框，anchor框，gt框。

​	在我们的实现中，回归使用的特征与特征图具有相同的空间大小。为了解决多个大小的问题，学习k个bb回归器。每个回归器对应于一种尺度，k个回归器不共享权重。

#### 3.1.3 Training RPNs

​	我们使用“image-centric”采样策略来训练该网络。每个mini-batch来源于一幅图像（多个pos和neg anchor）。我们每幅图随机采样256个anchor，来计算一个mini-batch的loss，pos:neg=1:1。如果一幅图像不足128个pos样本，则使用neg样本填充mini-batch。

​	新层使用零均值，标准差0.01的高斯分布。其他层使用预训练参数。lr = 0.001，60k个mini-batch，lr=0.0001，20k个mini-batch。momentum0.9，weight decay 0.0005。

#### Sharing Features for RPN and Fast R-CNN

​	我们使用Fast R-CNN作为检测网络。我们讨论三种方法来训练两个网络，并共享特征：

（1）*Alternating training.* 我们首先训练RPN，然后使用proposal来训练Fast R-CNN。Fast R-CNN微调网络，接着用于初始化RPN，如此迭代。

（2）Approximate joint training. 如图2，训练时RPN和Fast R-CNN网络融合到一个网络。在每次迭代中，前传产生region proposal，作固定的，预先计算的proposal来训练Fast R-CNN。这种方法很容易实现，但是该方法忽略了proposal 坐标的导数，因此是近似的。在我们的实验中，我们发现可以产生差不多的结果，并且减少了25-50%的训练时间。

（3）*Non-approximate joint training.* Fast R-CNN中的RoI pooling层接受卷积特征，也预测bb作为输入。

**4-Step Alternating Training. ** 本文采用4步训练算法，通过交替优化来学习共享特征。1）训练RPN；2）RPN生成proposal，用于训练Fast R-CNN；3）使用Fast R-CNN网络初始化RPN训练，但是固定共享卷积层，fine-tune RPN单独的层。4）固定共享巻积层，fine-tune Fast R-CNN单独层。

#### Implementation Details

![t1](images\t1.png)

​	re-scale图像，短边为600像素。ZF和VGG stride为16像素。对于anchor，我们使用三个尺度$128^2$、$256^2$、$512^2$，三个比例，1:1、1:2、2:1。图3展示了我们方法在多尺度上的能力。表1展示了使用ZFnet，每个anchor平均proposal大小。在训练时忽略跨越边界的anchor。对于1000x600的图像，大约2000（60x40x9）个anchor。忽略边界的，大约6000个anchor用于训练。在测试时，我们剪切边界proposal到图像边界。基于cls得分，使用NMS，阈值为0.7。每幅图大约剩下2000个proposal。NMS之后，使用最高得分的N个proposal区域用于检测。使用2000个RPN proposal来训练Fast R-CNN。

### Experiments

![t2](images\t2.png)

![t3](images\t3.png)

![t4](images\t4.png)

![t5](images\t5.png)

![t6](images\t6.png)

![t7](images\t7.png)

![t8](images\t8.png)

![t9](images\t9.png)

![t10](images\t10.png)

![t11](images\t11.png)

![t12](images\t12.png)

![f4](images\f4.png)

![f5](images\f5.png)

![f6](images\f6.png)